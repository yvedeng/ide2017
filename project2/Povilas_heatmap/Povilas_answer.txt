How big data sets are reasonable to load in d3 visualizations? What types of visualizations does a limit like that exclude? Give examples of visualizations that must be based on large data sets and discuss how they deal with this. Could you have reduced the size of the data sets in this assignment? 

It is hard to answer what is reasonable but probbably it is up to 5-10megabytes. Obviously, the smaller it is - the better because every user will have to load everything. d3.js users in forums say that you can load 100mb or maybe even 1Gb files but that would be pushing the limits.
E.g. One of our data files for this assignment was about 10-15kb, if we would like to create world map with switching temperatures based on years set years, we would need at least 1000-2000 locations, which would scales size to 20-30mb. Of course visualization itself wouldn't be very complicated, so that would be still reasonable if you have normal internet connections.
There are various methods to deal with big data sets: using Canvas instead of SVG/DOM could reduce complexity. CSV or TSV files are lighter than JSON. Well structured code, reusing functions and similar things can help slightly. Other thing is simply cropping your data, at some point it gets unreasonable to try showing every single point, it becomes a mess. Using libraries like Crossfilter helps as well.
Yes, we didn't use all the points from every dataset, so cropping those would reduce size.